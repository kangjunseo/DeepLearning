{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDrihTJXcQre42Qm8DICo+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kangjunseo/DeepLearning/blob/main/S_Pred/S_pred_ASA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# S_pred_ASA\n",
        "공부용 코드라 주석이 필요 이상으로 많을 수 있습니다.\n",
        "난잡해 보일 수는 있지만, 주석만으로도 논문의 주요 흐름을 파악할 수 있을 정도로 자세히 적어놓았습니다."
      ],
      "metadata": {
        "id": "CKxm6fHWr_cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Requirements"
      ],
      "metadata": {
        "id": "L5tQUhe2oyjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops fair-esm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwyTzNzXon2O",
        "outputId": "c9883b6f-57f5-45bb-9995-3d3510be33de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: fair-esm in /usr/local/lib/python3.8/dist-packages (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "k6u-ENQOqcIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import esm\n",
        "import json\n",
        "import numpy as np\n",
        "import argparse\n",
        "from einops import rearrange\n",
        "import string"
      ],
      "metadata": {
        "id": "0WlI2H78omr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4B-bQ526JVu",
        "outputId": "e03f103d-aeed-4dc7-e189-c2c2f90b84cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining models and functions"
      ],
      "metadata": {
        "id": "KxxqEKE3qdxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROTEIN_PROPERTY = \"accessible surface area\"\n",
        "ASA_NORM_SCALE = 200  # at ASA dataset, divide 200 to make values smaller\n",
        "\n",
        "MAX_MSA_ROW_NUM = 256  # if larger than 256 sequences, downscale to 256\n",
        "MAX_MSA_COL_NUM = 1023  # skip proteins bigger than 1023 residues\n",
        "\n",
        "torch.set_grad_enabled(False)  # autograd off -> memory usage down, calculate speed up"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyzaJBjlqYNu",
        "outputId": "ef4bd66a-9821-40d2-eeab-23dd55886370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f55d71947c0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class lstm_net(nn.Module):\n",
        "    def __init__(self, input_feature_size=768, hidden_node=256, dropout=0.25, need_row_attention=False, class_num=8):\n",
        "        super().__init__()\n",
        "        self.need_row_attention = need_row_attention\n",
        "        self.linear_proj = nn.Sequential(  # MSA features MLP\n",
        "            nn.Linear(input_feature_size, input_feature_size // 2),  # 768 -> 384\n",
        "            nn.InstanceNorm1d(input_feature_size // 2), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_feature_size // 2, input_feature_size // 4),  # 384 -> 192\n",
        "            nn.InstanceNorm1d(input_feature_size // 4), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_feature_size // 4, input_feature_size // 4),  # 192 -> 192\n",
        "        )\n",
        "\n",
        "        if self.need_row_attention:\n",
        "            lstm_input_feature_size = input_feature_size // 4 + 144*2\n",
        "        else:\n",
        "            lstm_input_feature_size = input_feature_size // 4\n",
        "\n",
        "        self.lstm = nn.LSTM(  # LSTM layer\n",
        "            input_size=lstm_input_feature_size,\n",
        "            hidden_size=hidden_node,  # 256 hidden unit\n",
        "            num_layers=2,  # 2 LSTM layers\n",
        "            bidirectional=True,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        self.to_property = nn.Sequential(  # final classification layer\n",
        "            nn.Linear(hidden_node * 2, hidden_node * 2),\n",
        "            nn.InstanceNorm1d(hidden_node * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_node * 2, class_num),\n",
        "        )\n",
        "\n",
        "        def forward(self, msa_query_embeddings, msa_row_attentions):\n",
        "            msa_query_embeddings = self.linear_proj(msa_query_embeddings)  # input : last attention block(r x c x 768) output tensor\n",
        "\n",
        "            if self.need_row_attention:  # input : row attention maps\n",
        "                msa_row_attentions = rearrange(msa_row_attentions, 'b l h i j -> b (l h) i j')  # rearrange dimension\n",
        "                msa_attention_features = torch.cat((torch.mean(msa_row_attentions, dim=2), torch.mean(msa_row_attentions, dim=3)), dim=1)  # mean pooling\n",
        "                msa_attention_features = msa_attention_features.permute((0, 2, 1))  # transpose\n",
        "\n",
        "                lstm_input = torch.cat([msa_query_embeddings, msa_attention_features], dim=2)  # concat first and second feature\n",
        "\n",
        "            else:\n",
        "                lstm_input = msa_query_embeddings\n",
        "\n",
        "            lstm_input = lstm_input.permute((1,0,2))  # transpose\n",
        "            lstm_output, lstm_hidden = self.lstm(lstm_input)\n",
        "            lstm_output = lstm_output.permute((1,0,2))  # transpose\n",
        "            label_output = self.to_property(lstm_output)\n",
        "\n",
        "            return label_output"
      ],
      "metadata": {
        "id": "SZP7lhAms6rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_msa_file(filepath, msa_row_num):\n",
        "\n",
        "    seqs = []\n",
        "    table = str.maketrans(dict.fromkeys(string.ascii_lowercase))\n",
        "    with open(filepath,\"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    # read file line by line\n",
        "    for i in range(0,len(lines),2):\n",
        "\n",
        "        seq = []\n",
        "        seq.append(lines[i])\n",
        "        seq.append(lines[i+1].rstrip().translate(table))\n",
        "        seqs.append(seq)\n",
        "\n",
        "    if msa_row_num > MAX_MSA_ROW_NUM:\n",
        "        msa_row_num = MAX_MSA_ROW_NUM\n",
        "        print(f\"The MSA row num is larger than {MAX_MSA_ROW_NUM}. This program force the msa row to under {MAX_MSA_ROW_NUM}\")\n",
        "\n",
        "    seqs = seqs[: msa_row_num]\n",
        "    return seqs, seqs[0]"
      ],
      "metadata": {
        "id": "1ScoSXf_s7CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_msa_transformer_features(msa_seq, msa_transformer, msa_batch_converter, device=torch.device(\"cpu\")):\n",
        "    msa_seq_label, msa_seq_str, msa_seq_token = msa_batch_converter([msa_seq])\n",
        "    msa_seq_token = msa_seq_token.to(device)\n",
        "    msa_row, msa_col = msa_seq_token.shape[1], msa_seq_token.shape[2]\n",
        "    print(f\"{msa_seq_label[0][0]}, msa_row: {msa_row}, msa_col: {msa_col}\")\n",
        "\n",
        "    if msa_col > MAX_MSA_COL_NUM:\n",
        "        print(f\"msa col num should less than {MAX_MSA_COL_NUM}. This program force the msa col to under {MAX_MSA_COL_NUM}\")\n",
        "    msa_seq_token = msa_seq_token[:, :, :MAX_MSA_COL_NUM]\n",
        "\n",
        "    ### keys: ['logits', 'representations', 'col_attentions', 'row_attentions', 'contacts']\n",
        "    msa_transformer_outputs = msa_transformer(\n",
        "        msa_seq_token, repr_layers=[12],\n",
        "        need_head_weights=True, return_contacts=True)\n",
        "    msa_row_attentions = msa_transformer_outputs['row_attentions']\n",
        "    msa_representations = msa_transformer_outputs['representations'][12]\n",
        "    msa_query_representation = msa_representations[:, 0, 1:, :]  # remove start token\n",
        "    msa_row_attentions = msa_row_attentions[..., 1:, 1:]  # remove start token\n",
        "\n",
        "    return msa_query_representation, msa_row_attentions\n"
      ],
      "metadata": {
        "id": "7Xpf3icJHE4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_property_to_json(out_property_json, output_property, query_seq):\n",
        "    output_property_list = output_property.tolist()\n",
        "    output_property_list = [round(x) for x in output_property_list]\n",
        "\n",
        "    json_dict = {\n",
        "        \"asa_data\": output_property_list,\n",
        "        \"query_seq\": query_seq,\n",
        "        \"metadata\": {\n",
        "            \"precision\": 4,\n",
        "            \"title\": \"accessible surface area\",\n",
        "            \"data-min\": 0,\n",
        "            \"data-max\": 200,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(out_property_json, \"w\") as f:\n",
        "        json.dump(json_dict, f, indent=4)"
      ],
      "metadata": {
        "id": "aPAW-TBQKxCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main code"
      ],
      "metadata": {
        "id": "ZmHzdj_SqZgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    print(\"gpu is not available, run on cpu\")\n",
        "    device = torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "id": "OiLmzLr_pDUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = '/content/drive/MyDrive/S_pred/examples/s_pred_asa.a3m'\n",
        "output_path = '/content/drive/MyDrive/S_pred/s_pred_asa.out'\n",
        "conv_model_path = '/content/drive/MyDrive/S_pred/s_pred_asa_weights.pth'"
      ],
      "metadata": {
        "id": "VNbLiLQF1Zgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msa_transformer, msa_alphabet = esm.pretrained.esm_msa1_t12_100M_UR50S()\n",
        "msa_batch_converter = msa_alphabet.get_batch_converter()\n",
        "\n",
        "msa_transformer.to(device)\n",
        "msa_transformer.eval()"
      ],
      "metadata": {
        "id": "7hnwkd7PqFio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model = lstm_net(input_feature_size=768, hidden_node=256, dropout=0.25, need_row_attention=True, class_num=1)  # class num=1 because ASA\n",
        "conv_model = conv_model.to(device)"
      ],
      "metadata": {
        "id": "kj442GX3qTcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if device.type == 'cpu':\n",
        "    ch = torch.load(conv_model_path, map_location=torch.device('cpu'))\n",
        "else:\n",
        "    ch = torch.load(conv_model_path)"
      ],
      "metadata": {
        "id": "kwAKcRLd0-Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model.load_state_dict(ch['conv_model'])\n",
        "conv_model.to(device)\n",
        "conv_model.eval()\n",
        "\n",
        "for param in msa_transformer.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in conv_model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "mc5S-ebfEvjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msa_seq, query_seq = read_msa_file(input_path, 256)\n",
        "msa_row_num = len(msa_seq)\n",
        "msa_col_num = len(query_seq)\n",
        "\n",
        "print(f\"msa row number: {msa_row_num}\")\n",
        "print(f\"msa column number: {msa_col_num}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTMwGXhwE7Tt",
        "outputId": "7e0cc89a-969d-458b-e85a-aa4073e6b681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "msa row number: 256\n",
            "msa column number: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msa_query_representation, msa_row_attentions = extract_msa_transformer_features(msa_seq,\n",
        "                                                                                msa_transformer,\n",
        "                                                                                msa_batch_converter,\n",
        "                                                                                device=device)\n",
        "msa_query_representation.to(device)\n",
        "msa_row_attentions.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "fbIlfuSrGrCA",
        "outputId": "c61a35c3-2240-442e-f735-716dbc349eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">5G6UA\n",
            ", msa_row: 256, msa_col: 262\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-b61f4706fba0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m msa_query_representation, msa_row_attentions = extract_msa_transformer_features(msa_seq,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                                                 \u001b[0mmsa_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                                 \u001b[0mmsa_batch_converter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                                 device=device)\n\u001b[1;32m      5\u001b[0m \u001b[0mmsa_query_representation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-5514ff1b6463>\u001b[0m in \u001b[0;36mextract_msa_transformer_features\u001b[0;34m(msa_seq, msa_transformer, msa_batch_converter, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m### keys: ['logits', 'representations', 'col_attentions', 'row_attentions', 'contacts']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     msa_transformer_outputs = msa_transformer(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mmsa_seq_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         need_head_weights=True, return_contacts=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/esm/model/msa_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_head_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;31m# col_attentions: B x L x H x C x R x R\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mcol_attentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_attn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0;31m# row_attentions: B x L x H x C x C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mrow_attentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_attn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.21 GiB (GPU 0; 14.76 GiB total capacity; 10.36 GiB already allocated; 3.28 GiB free; 10.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msa_query_representation.to(device)\n",
        "msa_row_attentions.to(device)\n",
        "\n",
        "output_property = conv_model(msa_query_representation, msa_row_attentions)\n",
        "output_property = output_property.squeeze(dim=2)\n",
        "\n",
        "\n",
        "output_property_np = output_property.data.cpu().numpy().squeeze()\n",
        "output_property_np[output_property_np < 0] = 0\n",
        "output_property_np = output_property_np*ASA_NORM_SCALE\n",
        "\n",
        "output_property_json_path = args.output_path + '.asa.json'\n",
        "save_property_to_json(output_property_json_path, output_property_np, query_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "pqsER7WTHpV3",
        "outputId": "608fa3a0-bb0c-4300-e0b0-bed13c410766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-003475303aef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmsa_query_representation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmsa_row_attentions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_property\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsa_query_representation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsa_row_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput_property\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_property\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'msa_query_representation' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0nzLs5NzKzRf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}